{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model \n",
    "## create training, validation and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features from pre processed data file\n",
    "df = pd.read_csv('data\\df_data_features.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I like to do is to shuffle the samples in case there was some order (e.g. all positive samples on top). Here n is the number of data points. random_state is a seed for the random number generator. This allows you to obtain reproducible results when sampling from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "df = df.sample(n=len(df), random_state = 42)\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "#extract 30% data and split them equally to validation and test samples. \n",
    "df_valid = df.sample(frac=0.30, random_state=42)\n",
    "df_train = df.drop(df_valid.index)\n",
    "df_test = df_valid.sample(frac=0.5, random_state=42)\n",
    "df_valid = df_valid.drop(df_test.index)\n",
    "\n",
    "print('test data size: %.3f'%(len(df_test)/len(df)))\n",
    "print('validation data size: %.3f'%(len(df_valid)/len(df)))\n",
    "print('training data size: %.3f'%(len(df_train)/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have we used all the data?\n",
    "print('sample count (n = %d)'%len(df))\n",
    "assert len(df) == (len(df_test)+len(df_valid)+len(df_train)),'Not all samples used.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.OUTPUT_LABEL.values)))\n",
    "print('Valid prevalence(n = %d):%.3f'%(len(df_valid),calc_prevalence(df_valid.OUTPUT_LABEL.values)))\n",
    "print('Train all prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the data ready to be dropped into predictive model?\n",
    "\n",
    "The dataset is  imbalanced with more negatives than positives. So the model might just assign all samples as negative.\n",
    "\n",
    "Let me create a balanced training data set by sub-sampling. There may be other approaches to create a balanced training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the training data into positive and negative\n",
    "rows_pos = df_train.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train.loc[rows_pos]\n",
    "df_train_neg = df_train.loc[~rows_pos]\n",
    "\n",
    "# create balanced data by merging positive and equal number of negative data samples\n",
    "df_train_balanced = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n",
    "\n",
    "# shuffle the order of training samples \n",
    "df_train_balanced = df_train_balanced.sample(n = len(df_train_balanced), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "print('Balanced training data prevalence(n = %d):%.3f'%(len(df_train_balanced), calc_prevalence(df_train_balanced.OUTPUT_LABEL.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced.to_csv('data\\df_train_balanced.csv',index=False)\n",
    "df_train.to_csv('data\\df_train.csv',index=False)\n",
    "df_valid.to_csv('data\\df_valid.csv',index=False)\n",
    "df_test.to_csv('data\\df_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2use = df_train.columns.tolist()\n",
    "col2use.remove('OUTPUT_LABEL')\n",
    "col2use.remove('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get features values for model training \n",
    "X_train_all = df_train[col2use].values\n",
    "X_train = df_train_balanced[col2use].values\n",
    "X_valid = df_valid[col2use].values\n",
    "\n",
    "y_train = df_train_balanced['OUTPUT_LABEL'].values\n",
    "y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "\n",
    "print('Training All shapes:',X_train_all.shape)\n",
    "print('Training shapes:',X_train.shape, y_train.shape)\n",
    "print('Validation shapes:',X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset are of different scales. Before feeding the data to a machine learning model, this data should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "X_train_tf = scaler.transform(X_train)\n",
    "X_valid_tf = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the scaler to use it with test data\n",
    "import pickle\n",
    "pickle.dump(scaler, open('data\\scaler.sav', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh=0.5):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh=0.5):\n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors = 100)\n",
    "knn.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_knn = knn.predict(X_valid_tf)\n",
    "\n",
    "y_train_preds = knn.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_knn = knn.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('KNN')\n",
    "print('Training:')\n",
    "thresh = 0.5\n",
    "knn_train_auc, knn_train_accuracy, knn_train_recall, \\\n",
    "    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "knn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n",
    "    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds_knn, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc,roc_auc_score\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_preds_knn)\n",
    "fp_rate, tp_rate, thresholds = roc_curve(y_valid, y_valid_preds_knn)\n",
    "plt.figure()\n",
    "plt.plot(fp_rate, tp_rate, label='KNN (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC - KNN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#plot confusion matrix\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix(y_valid, Y_knn), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Greens_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "title = 'Accuracy Score: {0}'.format(knn.score(X_valid_tf , y_valid))\n",
    "plt.title(title, size = 12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the summary of classification\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_valid, Y_knn, target_names = ['NO', 'YES']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgdc = SGDClassifier(loss='log_loss', random_state=42)\n",
    "sgdc.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sgdc=sgdc.predict(X_valid_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_sgdc = sgdc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Stochastic Gradient Descend')\n",
    "print('Training:')\n",
    "sgdc_train_auc, sgdc_train_accuracy, sgdc_train_recall, sgdc_train_precision, sgdc_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "sgdc_valid_auc, sgdc_valid_accuracy, sgdc_valid_recall, sgdc_valid_precision, sgdc_valid_specificity = print_report(y_valid,y_valid_preds_sgdc, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix(y_valid, Y_sgdc), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Greens_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(sgdc.score(X_valid_tf , y_valid))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth = 10, random_state = 42)\n",
    "tree.fit(X_train_tf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tree=tree.predict(X_valid_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = tree.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_tree = tree.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Decision Tree')\n",
    "print('Training:')\n",
    "tree_train_auc, tree_train_accuracy, tree_train_recall, tree_train_precision, tree_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "tree_valid_auc, tree_valid_accuracy, tree_valid_recall, tree_valid_precision, tree_valid_specificity = print_report(y_valid,y_valid_preds_tree, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,roc_curve, auc,roc_auc_score\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_preds_tree)\n",
    "fp_rate, tp_rate, thresholds = roc_curve(y_valid, y_valid_preds_tree)\n",
    "plt.figure()\n",
    "plt.plot(fp_rate, tp_rate, label='Decision Tree (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC -  Decision Trees')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix(y_valid, Y_tree), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Greens_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(tree.score(X_valid_tf , y_valid))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(max_depth = 6, random_state = 42)\n",
    "rf.fit(X_train_tf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rf=rf.predict(X_valid_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = rf.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_rf = rf.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Random Forest')\n",
    "print('Training:')\n",
    "rf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision, rf_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "rf_valid_auc, rf_valid_accuracy, rf_valid_recall, rf_valid_precision, rf_valid_specificity = print_report(y_valid,y_valid_preds_rf, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,roc_curve, auc,roc_auc_score\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_preds_rf)\n",
    "fp_rate, tp_rate, thresholds = roc_curve(y_valid, y_valid_preds_rf)\n",
    "plt.figure()\n",
    "plt.plot(fp_rate, tp_rate, label='Random Forest area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC -  Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix(y_valid, Y_rf), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(rf.score(X_valid_tf , y_valid))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc =GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=3, random_state=42)\n",
    "gbc.fit(X_train_tf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_gbc=gbc.predict(X_valid_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_gbc = gbc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Gradient Boosting Classifier')\n",
    "print('Training:')\n",
    "gbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision, gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "gbc_valid_auc, gbc_valid_accuracy, gbc_valid_recall, gbc_valid_precision, gbc_valid_specificity = print_report(y_valid,y_valid_preds_gbc, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix(y_valid, Y_gbc), annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(gbc.score(X_valid_tf , y_valid))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({'classifier':['KNN','KNN','SGDC','SGDC','DT','DT','RF','RF','GBC','GBC'],\n",
    "                           'data_set':['train','valid']*5,\n",
    "                          'auc':[knn_train_auc, knn_valid_auc,sgdc_train_auc,sgdc_valid_auc,tree_train_auc,tree_valid_auc,rf_train_auc,rf_valid_auc,gbc_train_auc,gbc_valid_auc],\n",
    "                          'accuracy':[knn_train_accuracy, knn_valid_accuracy,sgdc_train_accuracy,sgdc_valid_accuracy,tree_train_accuracy,tree_valid_accuracy,rf_train_accuracy,rf_valid_accuracy,gbc_train_accuracy,gbc_valid_accuracy],\n",
    "                          'recall':[knn_train_recall, knn_valid_recall,sgdc_train_recall,sgdc_valid_recall,tree_train_recall,tree_valid_recall,rf_train_recall,rf_valid_recall,gbc_train_recall,gbc_valid_recall],\n",
    "                          'precision':[knn_train_precision, knn_valid_precision,sgdc_train_precision,sgdc_valid_precision,tree_train_precision,tree_valid_precision,rf_train_precision,rf_valid_precision,gbc_train_precision,gbc_valid_precision],\n",
    "                          'specificity':[knn_train_specificity, knn_valid_specificity,sgdc_train_specificity,sgdc_valid_specificity,tree_train_specificity,tree_valid_specificity,rf_train_specificity,rf_valid_specificity,gbc_train_specificity,gbc_valid_specificity]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=df_results)\n",
    "fontsize=12\n",
    "ax.set_xlabel('Classifier',fontsize = fontsize)\n",
    "ax.set_ylabel('AUC', fontsize = fontsize)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., fontsize = fontsize)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"classifier\", y=\"recall\", hue=\"data_set\", data=df_results)\n",
    "ax.set_xlabel('Classifier',fontsize = fontsize)\n",
    "ax.set_ylabel('Recall', fontsize = fontsize)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., fontsize = fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"classifier\", y=\"precision\", hue=\"data_set\", data=df_results)\n",
    "ax.set_xlabel('Classifier',fontsize = fontsize)\n",
    "ax.set_ylabel('Precision', fontsize = fontsize)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., fontsize = fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"classifier\", y=\"specificity\", hue=\"data_set\", data=df_results)\n",
    "ax.set_xlabel('Classifier',fontsize = fontsize)\n",
    "ax.set_ylabel('Specificity', fontsize = fontsize)\n",
    "ax.tick_params(labelsize=fontsize)\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., fontsize = fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "models = ['KNN','Stochastic Gradient Descent Classifier','Decision Tree','Random Forest','Gradient Bossting Classifier' ]\n",
    "model = np.arange(len(models))\n",
    "values = [knn_train_auc, sgdc_train_auc,  tree_train_auc,rf_train_auc, gbc_train_auc]\n",
    "plt.bar(model, values, align='center', width = 0.15, alpha=0.7, color = 'red', label= 'AUC')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "values = [knn_train_accuracy, sgdc_train_accuracy,tree_train_accuracy, rf_train_accuracy,gbc_train_accuracy]\n",
    "plt.bar(model+0.15, values, align='center', width = 0.15, alpha=0.7, color = 'blue', label = 'auccuracy')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "values = [knn_train_recall, sgdc_train_recall,  tree_train_recall,rf_train_recall,gbc_train_recall]\n",
    "plt.bar(model+0.3, values, align='center', width = 0.15, alpha=0.7, color = 'green', label = 'recall')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "#ax.invert_yaxis()\n",
    "values = [knn_train_precision,sgdc_train_precision, tree_train_precision, rf_train_precision,gbc_train_precision]\n",
    "plt.bar(model+0.45, values, align='center', width = 0.15, alpha=0.7, color = 'orange', label = 'precision')\n",
    "plt.xticks(model, models,rotation=90)\n",
    "\n",
    "values = [knn_train_specificity, sgdc_train_specificity, tree_train_specificity, rf_train_specificity,gbc_train_specificity]\n",
    "plt.bar(model+0.60, values, align='center', width = 0.15, alpha=0.7, color = 'black', label = 'specifity')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Performance of different models on training data')\n",
    "# removing the axis on the top and right of the plot window\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "models = ['KNN','Stochastic Gradient Descent Classifier','Decision Tree','Random Forest','Gradient Bossting Classifier' ]\n",
    "values = [knn_valid_auc, sgdc_valid_auc, tree_valid_auc, rf_valid_auc, gbc_valid_auc]\n",
    "model = np.arange(len(models))\n",
    "plt.bar(model, values, align='center', width = 0.15, alpha=0.7, color = 'red', label= 'AUC')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "values = [knn_valid_accuracy,  sgdc_valid_accuracy, tree_valid_accuracy,rf_valid_accuracy, gbc_valid_accuracy]\n",
    "model = np.arange(len(models))\n",
    "plt.bar(model+0.15, values, align='center', width = 0.15, alpha=0.7, color = 'blue', label = 'auccuracy')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "values = [knn_train_recall, sgdc_train_recall, tree_train_recall, rf_train_recall, gbc_train_recall]\n",
    "model = np.arange(len(models))\n",
    "plt.bar(model+0.3, values, align='center', width = 0.15, alpha=0.7, color = 'green', label = 'recall')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "values = [knn_valid_precision, sgdc_valid_precision, tree_valid_precision, rf_valid_precision, gbc_valid_precision]\n",
    "model = np.arange(len(models))\n",
    "plt.bar(model+0.45, values, align='center', width = 0.15, alpha=0.7, color = 'orange', label = 'precision')\n",
    "plt.xticks(model, models,rotation=90)\n",
    "\n",
    "values = [knn_valid_specificity, sgdc_valid_specificity, tree_valid_specificity, rf_valid_specificity, gbc_valid_specificity]\n",
    "model = np.arange(len(models))\n",
    "plt.bar(model+0.60, values, align='center', width = 0.15, alpha=0.7, color = 'black', label = 'specifity')\n",
    "plt.xticks(model, models)\n",
    "\n",
    "plt.title('Performance of different models on validation data')\n",
    "plt.ylabel('Metric')\n",
    "    \n",
    "# removing the axis on the top and right of the plot window\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_knn, tpr_knn, thresholds = roc_curve(y_valid, y_valid_preds_knn)#knn\n",
    "fpr_sgdc, tpr_sgdc, thresholds = roc_curve(y_valid,y_valid_preds_sgdc )#SGD tree\n",
    "fpr_dt, tpr_dt, thresholds = roc_curve(y_valid,y_valid_preds_tree )#decision tree\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(y_valid, y_valid_preds_rf)#random forest classifier\n",
    "fpr_gbc, tpr_gbc, thresholds = roc_curve(y_valid,y_valid_preds_gbc )#gbc tree\n",
    "\n",
    "roc_auc_knn = roc_auc_score(y_valid, y_valid_preds_knn)\n",
    "roc_auc_sgdc = roc_auc_score(y_valid, y_valid_preds_sgdc)\n",
    "roc_auc_tree = roc_auc_score(y_valid, y_valid_preds_tree)\n",
    "roc_auc_rf = roc_auc_score(y_valid, y_valid_preds_rf)\n",
    "roc_auc_gbc = roc_auc_score(y_valid, y_valid_preds_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw ROC curve of different models\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.plot(fpr_knn, tpr_knn, label='KNN area = %0.2f)' % roc_auc_knn)\n",
    "plt.plot(fpr_sgdc, tpr_sgdc, label='SGDC area = %0.2f)' % roc_auc_sgdc)\n",
    "plt.plot(fpr_dt, tpr_dt, label='Decision Tree area = %0.2f)' % roc_auc_tree)\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest area = %0.2f)' % roc_auc_rf)\n",
    "plt.plot(fpr_gbc, tpr_gbc, label='GBC area = %0.2f)' % roc_auc_gbc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='random', alpha=.8)\n",
    "plt.xticks(np.arange(0,1.1,0.1))\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from learn_curve import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Random Forest)\"\n",
    "# Cross validation with 5 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.15, random_state=42)\n",
    "estimator = RandomForestClassifier(max_depth = 6, random_state = 42)\n",
    "plot_learning_curve(estimator, title, X_train_tf, y_train, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Stochastic Gradient Descent)\"\n",
    "# Cross validation with 5 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.15, random_state=42)\n",
    "estimator = SGDClassifier(loss='log_loss',random_state = 42)\n",
    "plot_learning_curve(estimator, title, X_train_tf, y_train, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = col2use,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 50\n",
    "ylocs = np.arange(num)\n",
    "# get the feature importance for top num and sort in reverse order\n",
    "values_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\n",
    "feature_labels = list(feature_importances.iloc[:num].index)[::-1]\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\n",
    "plt.barh(ylocs, values_to_plot, align = 'center')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Feature Importance Score - Random Forest')\n",
    "plt.yticks(ylocs, feature_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(tree.feature_importances_,\n",
    "                                   index = col2use,\n",
    "                                    columns=['importance']).sort_values('importance',\n",
    "                                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 50\n",
    "ylocs = np.arange(num)\n",
    "# get the feature importance for top num and sort in reverse order\n",
    "values_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\n",
    "feature_labels = list(feature_importances.iloc[:num].index)[::-1]\n",
    "\n",
    "plt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\n",
    "plt.barh(ylocs, values_to_plot, align = 'center')\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Feature Importance Score - Random Forest')\n",
    "plt.yticks(ylocs, feature_labels)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most important variables for random forest and decision tree are continuous variables. It may be because they can split many times compared to categorical variables.\n",
    "\n",
    "It may be worth reducing the number of variables to top N positive and negative features or use PCA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Hyperparameter tuning is about optimizing the parameters used in the models. For example, what is the maximum depth for your random forest? \n",
    "\n",
    "I will use Grid search to test all possible combinations over a grid of values. This is very computationally intensive. Most of this section is based on this medium blog post (https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)by William Koehrsen. \n",
    "\n",
    "I will optimize the hyper parameters for stochastic gradient descent, random forest and gradient boosting classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# number of trees\n",
    "n_estimators = range(200,1000,200)\n",
    "# maximum depth of the tree\n",
    "max_depth = range(1,10,1)\n",
    "# minimum number of samples to split a node\n",
    "min_samples_split = range(2,10,2)\n",
    "# criterion for evaluating a split\n",
    "criterion = ['gini','entropy']\n",
    "\n",
    "# random grid\n",
    "\n",
    "random_grid = {'n_estimators':n_estimators,\n",
    "              'max_depth':max_depth,\n",
    "              'min_samples_split':min_samples_split,\n",
    "              'criterion':criterion}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use auc to evaluate hyperparameters using RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "auc_scoring = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the randomized search cross-validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 20, cv = 5, scoring=auc_scoring,\n",
    "                               verbose = 1, random_state = 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scoring = evaluation metric used to pick the best model\n",
    "* n_iter = number of different combinations\n",
    "* cv = number of cross-validation splits\n",
    "\n",
    "Increasing n_iter and cv will increase run-time and decrease chance of overfitting. Note that the number of variables and grid size also influences the runtime. Cross-validation is a technique for splitting the data multiple times to get a better estimate of the performance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# fit the random search model (this will take a few minutes)\n",
    "t1 = time.time()\n",
    "rf_random.fit(X_train_tf, y_train)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's analyze the performance of the best model compared to the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = rf.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Baseline Random Forest')\n",
    "rf_train_auc_base = roc_auc_score(y_train, y_train_preds)\n",
    "rf_valid_auc_base = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print('Training AUC:%.3f'%(rf_train_auc_base))\n",
    "print('Validation AUC:%.3f'%(rf_valid_auc_base))\n",
    "\n",
    "print('Optimized Random Forest')\n",
    "y_train_preds_random = rf_random.best_estimator_.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_random = rf_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "rf_train_auc = roc_auc_score(y_train, y_train_preds_random)\n",
    "rf_valid_auc = roc_auc_score(y_valid, y_valid_preds_random)\n",
    "\n",
    "print('Training AUC:%.3f'%(rf_train_auc))\n",
    "print('Validation AUC:%.3f'%(rf_valid_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['none','l2','l1']\n",
    "max_iter = range(100,500,100)\n",
    "alpha = [0.001,0.003,0.01,0.03,0.1,0.3]\n",
    "random_grid_sgdc = {'penalty':penalty,\n",
    "              'max_iter':max_iter,\n",
    "              'alpha':alpha}\n",
    "# create the randomized search cross-validation\n",
    "sgdc_random = RandomizedSearchCV(estimator = sgdc, param_distributions = random_grid_sgdc, \n",
    "                                 n_iter = 20, cv = 5, scoring=auc_scoring,verbose = 0, \n",
    "                                 random_state = 42)\n",
    "\n",
    "t1 = time.time()\n",
    "sgdc_random.fit(X_train_tf, y_train)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = sgdc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Baseline sgdc')\n",
    "sgdc_train_auc_base = roc_auc_score(y_train, y_train_preds)\n",
    "sgdc_valid_auc_base = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print('Training AUC:%.3f'%(sgdc_train_auc_base))\n",
    "print('Validation AUC:%.3f'%(sgdc_valid_auc_base))\n",
    "print('Optimized sgdc')\n",
    "y_train_preds_random = sgdc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_random = sgdc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\n",
    "sgdc_train_auc = roc_auc_score(y_train, y_train_preds_random)\n",
    "sgdc_valid_auc = roc_auc_score(y_valid, y_valid_preds_random)\n",
    "\n",
    "print('Training AUC:%.3f'%(sgdc_train_auc))\n",
    "print('Validation AUC:%.3f'%(sgdc_valid_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Optimize gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees\n",
    "n_estimators = range(100,500,100)\n",
    "\n",
    "# maximum depth of the tree\n",
    "max_depth = range(1,5,1)\n",
    "\n",
    "# learning rate\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "\n",
    "# random grid\n",
    "\n",
    "random_grid_gbc = {'n_estimators':n_estimators,\n",
    "              'max_depth':max_depth,\n",
    "              'learning_rate':learning_rate}\n",
    "\n",
    "# create the randomized search cross-validation\n",
    "gbc_random = RandomizedSearchCV(estimator = gbc, param_distributions = random_grid_gbc,\n",
    "                                n_iter = 20, cv = 2, scoring=auc_scoring,\n",
    "                                verbose = 0, random_state = 42)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "gbc_random.fit(X_train_tf, y_train)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Baseline gbc')\n",
    "gbc_train_auc_base = roc_auc_score(y_train, y_train_preds)\n",
    "gbc_valid_auc_base = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "print('Training AUC:%.3f'%(gbc_train_auc_base))\n",
    "print('Validation AUC:%.3f'%(gbc_valid_auc_base))\n",
    "\n",
    "print('Optimized gbc')\n",
    "y_train_preds_random = gbc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds_random = gbc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\n",
    "gbc_train_auc = roc_auc_score(y_train, y_train_preds_random)\n",
    "gbc_valid_auc = roc_auc_score(y_valid, y_valid_preds_random)\n",
    "\n",
    "print('Training AUC:%.3f'%(gbc_train_auc))\n",
    "print('Validation AUC:%.3f'%(gbc_valid_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_results = pd.DataFrame({'classifier':['SGD','SGD','RF','RF','GB','GB'],\n",
    "                           'model':['base','optimized']*3,\n",
    "                          'auc':[sgdc_valid_auc_base,sgdc_valid_auc,\n",
    "                                 rf_valid_auc_base,rf_valid_auc,\n",
    "                                 gbc_valid_auc_base,gbc_valid_auc,],\n",
    "                          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"model\", data=df_results)\n",
    "ax.set_xlabel('Classifier',fontsize = 15)\n",
    "ax.set_ylabel('AUC', fontsize = 15)\n",
    "ax.tick_params(labelsize=15)\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improvement on validation dataset. It looks like the best among the three is gradient boosting classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to use it on new data.\n",
    "import pickle\n",
    "pickle.dump(gbc_random.best_estimator_, open('model\\gradient_boost_classifier_trained.pkl', 'wb'),protocol = 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the performance of trained model test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[col2use].values\n",
    "y_test = df_test['OUTPUT_LABEL'].values\n",
    "\n",
    "scaler = pickle.load(open('data\\scaler.sav', 'rb'))\n",
    "X_test_tf = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = pickle.load(open('model\\gradient_boost_classifier_trained.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = trained_model.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = trained_model.predict_proba(X_valid_tf)[:,1]\n",
    "y_test_preds = trained_model.predict_proba(X_test_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "print('Training:')\n",
    "train_auc, train_accuracy, train_recall, train_precision, train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "valid_auc, valid_accuracy, valid_recall, valid_precision, valid_specificity = print_report(y_valid,y_valid_preds, thresh)\n",
    "print('Test:')\n",
    "test_auc, test_accuracy, test_recall, test_precision, test_specificity = print_report(y_test,y_test_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\n",
    "auc_train = roc_auc_score(y_train, y_train_preds)\n",
    "\n",
    "fpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_preds)\n",
    "\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\n",
    "auc_test = roc_auc_score(y_test, y_test_preds)\n",
    "\n",
    "plt.plot(fpr_train, tpr_train, 'r-',label ='Train AUC:%.3f'%auc_train)\n",
    "plt.plot(fpr_valid, tpr_valid, 'b-',label ='Valid AUC:%.3f'%auc_valid)\n",
    "plt.plot(fpr_test, tpr_test, 'g-',label ='Test AUC:%.3f'%auc_test)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
